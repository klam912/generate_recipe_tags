{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting datasets\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from transformers)\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.1-cp311-cp311-macosx_11_0_arm64.whl (167 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m291.0/291.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from transformers) (2.29.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl (393 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.4/393.4 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.1-cp311-cp311-macosx_11_0_arm64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pandas (from datasets)\n",
      "  Downloading pandas-2.2.1-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]<=2024.2.0,>=2023.1.0 (from datasets)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from pandas->datasets) (2.9.0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, safetensors, regex, pyyaml, pyarrow-hotfix, pyarrow, fsspec, filelock, dill, pandas, multiprocess, huggingface-hub, tokenizers, transformers, datasets\n",
      "Successfully installed datasets-2.18.0 dill-0.3.8 filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.21.4 multiprocess-0.70.16 pandas-2.2.1 pyarrow-15.0.1 pyarrow-hotfix-0.6 pytz-2024.1 pyyaml-6.0.1 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.2 tzdata-2024.1 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997568726539612}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(task='sentiment-analysis')\n",
    "classifier(\"We are very happy to show you the Transformers lirbary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 11312, 10320, 55558, 10158, 10114, 11391, 10855, 10103, 13299, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "encoding = tokenizer(\"We are hapy to show you the library\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Hugging face company is a good way to get a lot of attention.\\n\\nIf you're going to do this, it's best to start with a face that is a little different from your usual face. This way, people will know what's going on, and you can keep the attention on your face without looking like a clown on the cover of GQ.\\n\\n3. Make it your profile picture on social media and other places that people are likely to see it.\\n\\nThe most important thing to\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "prompt = \"Hugging face company is\"\n",
    "inputs = tokenizer(prompt, return_tensors = \"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, penalty_alpha = 0.6, top_k=4, max_new_tokens=100)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-large\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Generate a vegan Italian recipe with the following ingredients: tomatoes, basil, garlic.\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(input_ids, max_length=500, num_return_sequences=1)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Best Vietnamese Recipe to fit your vibe!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data collection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scrap the data now from a website!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "url = 'https://www.recipetineats.com/category/vietnamese-recipes/'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have parsed through the website HTML, let's get all of the recipe links using find_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.recipetineats.com/vietnamese-lettuce-wraps-with-peanut-sauce/', 'https://www.recipetineats.com/vietnamese-caramel-ginger-chicken/', 'https://www.recipetineats.com/vietnamese-pork-noodle-bowls/', 'https://www.recipetineats.com/vietnamese-lemongrass-pork-steaks/', 'https://www.recipetineats.com/chicken-banh-mi-vietnamese-sandwich/', 'https://www.recipetineats.com/red-vietnamese-fried-rice/', 'https://www.recipetineats.com/vietnamese-shaking-beef/', 'https://www.recipetineats.com/vietnamese-baked-chicken/', 'https://www.recipetineats.com/vietnamese-chicken-salad/', 'https://www.recipetineats.com/vietnamese-chicken-pho-soup-pho-ga/', 'https://www.recipetineats.com/vietnamese-rice-paper-rolls-spring-rolls/', 'https://www.recipetineats.com/vietnamese-pho-recipe/', 'https://www.recipetineats.com/vietnamese-top-10-best-street-food-ho-chi-minh-city/', 'https://www.recipetineats.com/vietnamese-chicken-noodle-bowl/', 'https://www.recipetineats.com/vietnamese-caramelised-pork-bowls/', 'https://www.recipetineats.com/banh-mi-vietnamese-sandwich/', 'https://www.recipetineats.com/vietnamese-pork-meatballs-banh-mi/', 'https://www.recipetineats.com/caramelised-vietnamese-shredded-beef/', 'https://www.recipetineats.com/bun-cha-vietnamese-meatballs/', 'https://www.recipetineats.com/vietnamese-coconut-caramel-chicken/']\n"
     ]
    }
   ],
   "source": [
    "recipe_links = soup.find_all('a', class_= 'entry-image-link')\n",
    "# Get the href attribute to store the link\n",
    "links = []\n",
    "for recipe in recipe_links:\n",
    "    links.append(recipe.get('href'))\n",
    "print(links)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting all of the links for all of the recipe, let's iterate through each recipe title to see if it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipe Title: Vietnamese Lettuce Wraps with Peanut Sauce\n",
      "Recipe Title: Vietnamese Caramel Ginger Chicken\n",
      "Recipe Title: Vietnamese lemongrass pork noodle bowls (bun thit nuong)\n",
      "Recipe Title: Vietnamese lemongrass pork steaks\n",
      "Recipe Title: Chicken Banh Mi (Vietnamese sandwich)\n",
      "Recipe Title: Red Vietnamese Fried Rice\n",
      "Recipe Title: Vietnamese Shaking Beef\n",
      "Recipe Title: Vietnamese Baked Chicken\n",
      "Recipe Title: Vietnamese Chicken Salad\n",
      "Recipe Title: Vietnamese Chicken Pho soup (Pho Ga)\n",
      "Recipe Title: Vietnamese Rice Paper Rolls\n",
      "Recipe Title: Vietnamese Pho recipe\n",
      "Recipe Title: {Pilot Travel Video!!} Top 10 BEST Street Food in Vietnam – Ho Chi Minh City\n",
      "Recipe Title: Vietnamese Noodles with Lemongrass Chicken\n",
      "Recipe Title: Vietnamese Caramelised Pork Bowls\n",
      "Recipe Title: Banh Mi ! (Vietnamese sandwich)\n",
      "Recipe Title: Pork Meatballs for Banh Mi\n",
      "Recipe Title: Caramelised Vietnamese Shredded Beef\n",
      "Recipe Title: Bun Cha (Vietnamese Meatballs!)\n",
      "Recipe Title: Vietnamese Coconut Caramel Chicken\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urljoin\n",
    "\n",
    "for link in links:\n",
    "    recipe_page = requests.get(link)\n",
    "\n",
    "    recipe_soup = BeautifulSoup(recipe_page.content, 'html.parser')\n",
    "\n",
    "    # Extract the recipe's title\n",
    "    recipe_title = recipe_soup.find('h1').get_text()\n",
    "    print(f'Recipe Title: {recipe_title}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we're able to iterate through each of the link, we can start create a pipeline to extracts the necessary information (ingredients, instructions, and title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_info(div_class, ul_class, extra_char_to_strip, soup):\n",
    "    divs = soup.find_all('div', class_= div_class)\n",
    "    list_of_info = []\n",
    "    for div in divs:\n",
    "        ul = div.find_next('ul', class_=ul_class)\n",
    "        for li in ul.find_all('li'):\n",
    "            list_of_info.append(li.get_text().strip(extra_char_to_strip))\n",
    "    return list_of_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "import pandas as pd\n",
    "# Create a dataframe storing all of the vietnamese recipes\n",
    "recipes_df = pd.DataFrame(columns=['Title', 'Ingredients', 'Instructions', 'Tags'])\n",
    "for link in links:\n",
    "    recipe_page = requests.get(link)\n",
    "\n",
    "    recipe_soup = BeautifulSoup(recipe_page.content, 'html.parser')\n",
    "\n",
    "    # Extract the recipe's title and add to df\n",
    "    recipe_title = recipe_soup.find('h1').get_text()\n",
    "\n",
    "    new_row = pd.DataFrame({'Title': [recipe_title], 'Ingredients': [extract_info('wprm-recipe-ingredient-group', 'wprm-recipe-ingredients','▢ ', recipe_soup)], 'Instructions': [extract_info('wprm-recipe-instruction-group', 'wprm-recipe-instructions','▢ ', recipe_soup)]})\n",
    "    recipes_df = pd.concat([recipes_df, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese Lettuce Wraps with Peanut Sauce</td>\n",
       "      <td>[300g / 10 oz peeled whole cooked prawns/shrim...</td>\n",
       "      <td>[Pickle first – Put the boiling water, salt an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese Caramel Ginger Chicken</td>\n",
       "      <td>[1 kg / 2 lb skinless chicken thigh fillets , ...</td>\n",
       "      <td>[Toss chicken with fish sauce and chilli, then...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese lemongrass pork noodle bowls (bun t...</td>\n",
       "      <td>[1 batch lemongrass marinated pork (it’s marin...</td>\n",
       "      <td>[Pickle – In a large bowl, dissolve the salt a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vietnamese lemongrass pork steaks</td>\n",
       "      <td>[500g/1 lb pork shoulder , skinless and bonele...</td>\n",
       "      <td>[Cut pork – Cut into 8 equal, thinnish slices ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicken Banh Mi (Vietnamese sandwich)</td>\n",
       "      <td>[2 medium carrots , peeled cut into 2-3mm / 1/...</td>\n",
       "      <td>[Pickle – In a large bowl, dissolve the salt a...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0         Vietnamese Lettuce Wraps with Peanut Sauce   \n",
       "1                  Vietnamese Caramel Ginger Chicken   \n",
       "2  Vietnamese lemongrass pork noodle bowls (bun t...   \n",
       "3                  Vietnamese lemongrass pork steaks   \n",
       "4              Chicken Banh Mi (Vietnamese sandwich)   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  [300g / 10 oz peeled whole cooked prawns/shrim...   \n",
       "1  [1 kg / 2 lb skinless chicken thigh fillets , ...   \n",
       "2  [1 batch lemongrass marinated pork (it’s marin...   \n",
       "3  [500g/1 lb pork shoulder , skinless and bonele...   \n",
       "4  [2 medium carrots , peeled cut into 2-3mm / 1/...   \n",
       "\n",
       "                                        Instructions Tags  \n",
       "0  [Pickle first – Put the boiling water, salt an...  NaN  \n",
       "1  [Toss chicken with fish sauce and chilli, then...  NaN  \n",
       "2  [Pickle – In a large bowl, dissolve the salt a...  NaN  \n",
       "3  [Cut pork – Cut into 8 equal, thinnish slices ...  NaN  \n",
       "4  [Pickle – In a large bowl, dissolve the salt a...  NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our df is almost done! Now, we just need to append tags for each of the food's description. To do so, we'll use NLP's library called spacy and download its pre-trained model. We'll use this model to find tags and filter out unecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenlam/anaconda3/envs/tf/lib/python3.11/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.4). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['funny', 'pho', 'dish', 'superstar', 'Vietnamese', 'food', 'bun', 'nuong', 'tastier', 'contrast', 'fresh', 'vegetables', 'herbs', 'delicious', 'meats', 'light', 'healthy', 'dull', 'big', 'bowl', 'delicious', 'chicken', 'version', 'years', 'ago', 'bun', 'ga', 'nuong', 'soon', 'pork', 'version', 'immediately', 'Wednesday', 'today', 'noodle', 'bowls', 'recipe', 'lemongrass', 'pork', 'like', 'streets', 'Vietnam']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English NLP model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# The food description\n",
    "text = (\"I find it funny that pho is the dish that’s become the superstar of Vietnamese food when bun thit nuong is tastier to me! \"\n",
    "        \"I adore the contrast of fresh vegetables and herbs with delicious grilled meats, that it’s light and healthy yet anything but dull. \"\n",
    "        \"It’s a big bowl of delicious, and I shared the chicken version many years ago (bun ga nuong). \"\n",
    "        \"And as soon as I cracked the pork version, I shared in immediately (just last Wednesday!). \"\n",
    "        \"And I’m back today with the noodle bowls recipe that is made using the lemongrass pork – just like you get on the streets of Vietnam!\")\n",
    "\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract tags with filtering\n",
    "filtered_tags = [token.text for token in doc \n",
    "                 if token.pos_ not in [\"PRON\", \"VERB\",\"AUX\", \"ADP\",\"SYM\",\"PREP\"]\n",
    "                 and not token.is_stop\n",
    "                 and not token.is_punct]\n",
    "\n",
    "# Print extracted tags\n",
    "print(filtered_tags)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this filtering, we can see that the tags found from this description are good. Let's add a column into the df called food description and apply this pipeline to append to the tags column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add an empty column calle Description\n",
    "recipes_df[\"Description\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vietnamese Lettuce Wraps with Peanut Sauce\n",
      "Vietnamese Caramel Ginger Chicken\n",
      "Vietnamese lemongrass pork noodle bowls (bun thit nuong)\n",
      "Vietnamese lemongrass pork steaks\n",
      "Chicken Banh Mi (Vietnamese sandwich)\n",
      "Red Vietnamese Fried Rice\n",
      "Vietnamese Shaking Beef\n",
      "Vietnamese Baked Chicken\n",
      "Vietnamese Chicken Salad\n",
      "Vietnamese Chicken Pho soup (Pho Ga)\n",
      "Vietnamese Rice Paper Rolls\n",
      "Vietnamese Pho recipe\n",
      "{Pilot Travel Video!!} Top 10 BEST Street Food in Vietnam – Ho Chi Minh City\n",
      "Vietnamese Noodles with Lemongrass Chicken\n",
      "Vietnamese Caramelised Pork Bowls\n",
      "Banh Mi ! (Vietnamese sandwich)\n",
      "Pork Meatballs for Banh Mi\n",
      "Caramelised Vietnamese Shredded Beef\n",
      "Bun Cha (Vietnamese Meatballs!)\n",
      "Vietnamese Coconut Caramel Chicken\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests \n",
    "recipes_df = pd.DataFrame(columns=['Title', 'Ingredients', 'Instructions', 'Tags', 'Description'])\n",
    "for link in links:\n",
    "    recipe_page = requests.get(link)\n",
    "\n",
    "    recipe_soup = BeautifulSoup(recipe_page.content, 'html.parser')\n",
    "\n",
    "    recipe_title = recipe_soup.find('h1').get_text()\n",
    "    print(recipe_title)\n",
    "    h2_heading = recipe_soup.find('h2', class_ = ['has-text-align-center wp-block-heading', 'wp-block-heading has-text-align-center'])\n",
    "    paragraphs_block = []\n",
    "    if h2_heading:\n",
    "        for sibling in h2_heading.find_next_siblings():\n",
    "            if sibling.name == \"h2\" or sibling.name == \"h3\":\n",
    "                break\n",
    "            if sibling.name == \"p\":\n",
    "                paragraphs_block.append(sibling.get_text().strip())\n",
    "    new_row = pd.DataFrame({'Title': [recipe_title], 'Ingredients': [' '.join(extract_info('wprm-recipe-ingredient-group', 'wprm-recipe-ingredients','▢ ', recipe_soup))], 'Instructions': [' '.join(extract_info('wprm-recipe-instruction-group', 'wprm-recipe-instructions','▢ ', recipe_soup))], 'Description': [' '.join(paragraphs_block)]})\n",
    "    recipes_df = pd.concat([recipes_df, new_row], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Ingredients</th>\n",
       "      <th>Instructions</th>\n",
       "      <th>Tags</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Vietnamese Lettuce Wraps with Peanut Sauce</td>\n",
       "      <td>300g / 10 oz peeled whole cooked prawns/shrimp...</td>\n",
       "      <td>Pickle first – Put the boiling water, salt and...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>These lettuce wraps are not strictly Vietnames...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Vietnamese Caramel Ginger Chicken</td>\n",
       "      <td>1 kg / 2 lb skinless chicken thigh fillets , c...</td>\n",
       "      <td>Toss chicken with fish sauce and chilli, then ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>When you see today’s recipe, you’re going to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vietnamese lemongrass pork noodle bowls (bun t...</td>\n",
       "      <td>1 batch lemongrass marinated pork (it’s marina...</td>\n",
       "      <td>Pickle – In a large bowl, dissolve the salt an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I find it funny that pho is the dish that’s be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vietnamese lemongrass pork steaks</td>\n",
       "      <td>500g/1 lb pork shoulder , skinless and boneles...</td>\n",
       "      <td>Cut pork – Cut into 8 equal, thinnish slices o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I’ve been wanting to recreate the chargrilled ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chicken Banh Mi (Vietnamese sandwich)</td>\n",
       "      <td>2 medium carrots , peeled cut into 2-3mm / 1/1...</td>\n",
       "      <td>Pickle – In a large bowl, dissolve the salt an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Banh Mi is a meat filled French baguette sandw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0         Vietnamese Lettuce Wraps with Peanut Sauce   \n",
       "1                  Vietnamese Caramel Ginger Chicken   \n",
       "2  Vietnamese lemongrass pork noodle bowls (bun t...   \n",
       "3                  Vietnamese lemongrass pork steaks   \n",
       "4              Chicken Banh Mi (Vietnamese sandwich)   \n",
       "\n",
       "                                         Ingredients  \\\n",
       "0  300g / 10 oz peeled whole cooked prawns/shrimp...   \n",
       "1  1 kg / 2 lb skinless chicken thigh fillets , c...   \n",
       "2  1 batch lemongrass marinated pork (it’s marina...   \n",
       "3  500g/1 lb pork shoulder , skinless and boneles...   \n",
       "4  2 medium carrots , peeled cut into 2-3mm / 1/1...   \n",
       "\n",
       "                                        Instructions Tags  \\\n",
       "0  Pickle first – Put the boiling water, salt and...  NaN   \n",
       "1  Toss chicken with fish sauce and chilli, then ...  NaN   \n",
       "2  Pickle – In a large bowl, dissolve the salt an...  NaN   \n",
       "3  Cut pork – Cut into 8 equal, thinnish slices o...  NaN   \n",
       "4  Pickle – In a large bowl, dissolve the salt an...  NaN   \n",
       "\n",
       "                                         Description  \n",
       "0  These lettuce wraps are not strictly Vietnames...  \n",
       "1  When you see today’s recipe, you’re going to d...  \n",
       "2  I find it funny that pho is the dish that’s be...  \n",
       "3  I’ve been wanting to recreate the chargrilled ...  \n",
       "4  Banh Mi is a meat filled French baguette sandw...  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipes_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have added the descriptions of each food, let's now generate tags for each description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
